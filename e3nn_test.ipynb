{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holywater2/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import e3nn\n",
    "import ase\n",
    "import ase.neighborlist\n",
    "import torch_geometric\n",
    "import torch_geometric.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dtype = torch.float64\n",
    "torch.set_default_dtype(default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignn.data import get_train_val_loaders\n",
    "from repo_utils.data_utils import mp\n",
    "import repo_utils.debbug_utils as deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Loading complete: the number of the loaded data is 6923\n",
      "dict_keys(['id', 'desc', 'formula', 'e_hull', 'gap pbe', 'mu_b', 'elastic anisotropy', 'bulk modulus', 'shear modulus', 'atoms', 'e_form'])\n"
     ]
    }
   ],
   "source": [
    "crystals_dict_list = mp.load_json_sample(pdirname=\"dataset\")\n",
    "print(crystals_dict_list[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarvis.core.atoms import Atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "type_encoding = {}\n",
    "crystals = []\n",
    "num_atom_types = 0\n",
    "for crystal_dict in crystals_dict_list:\n",
    "    target = crystal_dict[\"e_form\"]\n",
    "    crystal = Atoms.from_dict(crystal_dict[\"atoms\"]).ase_converter()\n",
    "    for atom in crystal.symbols:\n",
    "        if atom not in type_encoding:\n",
    "            type_encoding[atom] = num_atom_types\n",
    "            num_atom_types += 1\n",
    "    crystals.append([crystal,target])\n",
    "\n",
    "type_onehot = torch.eye(len(type_encoding))\n",
    "print(num_atom_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_energies = torch.randn(2, 1, 1)  # dummy energies for example\n",
    "radial_cutoff = 3.5\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for crystal, target in crystals:\n",
    "    # edge_src and edge_dst are the indices of the central and neighboring atom, respectively\n",
    "    # edge_shift indicates whether the neighbors are in different images / copies of the unit cell\n",
    "    edge_src, edge_dst, edge_shift = ase.neighborlist.neighbor_list(\"ijS\", a=crystal, cutoff=radial_cutoff, self_interaction=True)\n",
    "\n",
    "    data = torch_geometric.data.Data(\n",
    "        pos=torch.tensor(crystal.get_positions()),\n",
    "        lattice=torch.tensor(crystal.cell.array).unsqueeze(0),  # We add a dimension for batching\n",
    "        x=type_onehot[[type_encoding[atom] for atom in crystal.symbols]],  # Using \"dummy\" inputs of scalars because they are all C\n",
    "        edge_index=torch.stack([torch.LongTensor(edge_src), torch.LongTensor(edge_dst)], dim=0),\n",
    "        edge_shift=torch.tensor(edge_shift, dtype=default_dtype),\n",
    "        energy=target  # dummy energy (assumed to be normalized \"per atom\")\n",
    "    )\n",
    "\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[36, 85], edge_index=[2, 220], pos=[36, 3], lattice=[1, 3, 3], edge_shift=[220, 3], energy=-0.7881006506944451)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/compuworks/anaconda3/envs/alignn-segnn/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "batch_size = 48\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the lengths of each split\n",
    "num_samples = len(dataset)\n",
    "train_size = int(train_ratio * num_samples)\n",
    "val_size = int(val_ratio * num_samples)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders for each split\n",
    "train_loader = torch_geometric.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch_geometric.data.DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = torch_geometric.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1524, 85], edge_index=[2, 19418], pos=[1524, 3], lattice=[48, 3, 3], edge_shift=[19418, 3], energy=[48], batch=[1524], ptr=[49])\n",
      "tensor([ 0,  0,  0,  ..., 46, 46, 47])\n",
      "tensor([[ 0.1469,  1.5310,  9.5913],\n",
      "        [ 3.0665,  7.0898, 14.0848],\n",
      "        [ 1.0984,  2.8551,  6.0809],\n",
      "        ...,\n",
      "        [ 3.8533,  1.1292,  4.8165],\n",
      "        [ 6.9829,  3.3875,  2.0793],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "print(data)\n",
    "print(data.batch)\n",
    "print(data.pos)\n",
    "print(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.nn.models.v2103.gate_points_networks import SimpleNetwork\n",
    "from typing import Dict, Union\n",
    "import torch_scatter\n",
    "\n",
    "class SimplePeriodicNetwork(SimpleNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"The keyword `pool_nodes` is used by SimpleNetwork to determine\n",
    "        whether we sum over all atom contributions per example. In this example,\n",
    "        we want use a mean operations instead, so we will override this behavior.\n",
    "        \"\"\"\n",
    "        self.pool = False\n",
    "        if kwargs['pool_nodes'] == True:\n",
    "            kwargs['pool_nodes'] = False\n",
    "            kwargs['num_nodes'] = 1.\n",
    "            self.pool = True\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # Overwriting preprocess method of SimpleNetwork to adapt for periodic boundary data\n",
    "    def preprocess(self, data: Union[torch_geometric.data.Data, Dict[str, torch.Tensor]]) -> torch.Tensor:\n",
    "        if 'batch' in data:\n",
    "            batch = data['batch']\n",
    "        else:\n",
    "            batch = data['pos'].new_zeros(data['pos'].shape[0], dtype=torch.long)\n",
    "\n",
    "        edge_src = data['edge_index'][0]  # Edge source\n",
    "        edge_dst = data['edge_index'][1]  # Edge destination\n",
    "\n",
    "        # We need to compute this in the computation graph to backprop to positions\n",
    "        # We are computing the relative distances + unit cell shifts from periodic boundaries\n",
    "        edge_batch = batch[edge_src]\n",
    "        edge_vec = (data['pos'][edge_dst]\n",
    "                    - data['pos'][edge_src]\n",
    "                    + torch.einsum('ni,nij->nj', data['edge_shift'], data['lattice'][edge_batch]))\n",
    "\n",
    "        return batch, data['x'], edge_src, edge_dst, edge_vec\n",
    "\n",
    "    def forward(self, data: Union[torch_geometric.data.Data, Dict[str, torch.Tensor]]) -> torch.Tensor:\n",
    "        # if pool_nodes was set to True, use scatter_mean to aggregate\n",
    "        output = super().forward(data)\n",
    "        if self.pool == True:\n",
    "            return torch_scatter.scatter_mean(output, data.batch, dim=0)  # Take mean over atoms per example\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimplePeriodicNetwork(\n",
    "    irreps_in=\"85x0e\",  # One hot scalars (L=0 and even parity) on each atom to represent atom type\n",
    "    irreps_out=\"1x0e\",  # Single scalar (L=0 and even parity) to output (for example) energy\n",
    "    max_radius=radial_cutoff, # Cutoff radius for convolution\n",
    "    num_neighbors=10.0,  # scaling factor based on the typical number of neighbors\n",
    "    pool_nodes=True,  # We pool nodes to predict total energy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "n_epochs = 300\n",
    "metric =\"mae\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "learning_rate = 0.005\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimplePeriodicNetwork(\n",
       "  (mp): MessagePassing(\n",
       "    (layers): ModuleList(\n",
       "      (0): Compose(\n",
       "        (first): Convolution(\n",
       "          (sc): FullyConnectedTensorProduct(85x0e x 1x0e -> 150x0e+50x1o+50x2e | 12750 paths | 12750 weights)\n",
       "          (lin1): FullyConnectedTensorProduct(85x0e x 1x0e -> 85x0e | 7225 paths | 7225 weights)\n",
       "          (fc): FullyConnectedNet[10, 100, 255]\n",
       "          (tp): TensorProduct(85x0e x 1x0e+1x1o+1x2e -> 85x0e+85x1o+85x2e | 255 paths | 255 weights)\n",
       "          (lin2): FullyConnectedTensorProduct(85x0e+85x1o+85x2e x 1x0e -> 150x0e+50x1o+50x2e | 21250 paths | 21250 weights)\n",
       "          (lin3): FullyConnectedTensorProduct(85x0e+85x1o+85x2e x 1x0e -> 1x0e | 85 paths | 85 weights)\n",
       "        )\n",
       "        (second): Gate (150x0e+50x1o+50x2e -> 50x0e+50x1o+50x2e)\n",
       "      )\n",
       "      (1): Compose(\n",
       "        (first): Convolution(\n",
       "          (sc): FullyConnectedTensorProduct(50x0e+50x1o+50x2e x 1x0e -> 250x0e+50x1o+50x1e+50x2o+50x2e | 17500 paths | 17500 weights)\n",
       "          (lin1): FullyConnectedTensorProduct(50x0e+50x1o+50x2e x 1x0e -> 50x0e+50x1o+50x2e | 7500 paths | 7500 weights)\n",
       "          (fc): FullyConnectedNet[10, 100, 750]\n",
       "          (tp): TensorProduct(50x0e+50x1o+50x2e x 1x0e+1x1o+1x2e -> 150x0e+200x1o+100x1e+100x2o+200x2e | 750 paths | 750 weights)\n",
       "          (lin2): FullyConnectedTensorProduct(150x0e+200x1o+100x1e+100x2o+200x2e x 1x0e -> 250x0e+50x1o+50x1e+50x2o+50x2e | 67500 paths | 67500 weights)\n",
       "          (lin3): FullyConnectedTensorProduct(150x0e+200x1o+100x1e+100x2o+200x2e x 1x0e -> 1x0e | 150 paths | 150 weights)\n",
       "        )\n",
       "        (second): Gate (250x0e+50x1o+50x1e+50x2o+50x2e -> 50x0e+50x1o+50x1e+50x2o+50x2e)\n",
       "      )\n",
       "      (2): Compose(\n",
       "        (first): Convolution(\n",
       "          (sc): FullyConnectedTensorProduct(50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e -> 50x0o+250x0e+50x1o+50x1e+50x2o+50x2e | 22500 paths | 22500 weights)\n",
       "          (lin1): FullyConnectedTensorProduct(50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e -> 50x0e+50x1o+50x1e+50x2o+50x2e | 12500 paths | 12500 weights)\n",
       "          (fc): FullyConnectedNet[10, 100, 1350]\n",
       "          (tp): TensorProduct(50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e+1x1o+1x2e -> 100x0o+150x0e+300x1o+250x1e+250x2o+300x2e | 1350 paths | 1350 weights)\n",
       "          (lin2): FullyConnectedTensorProduct(100x0o+150x0e+300x1o+250x1e+250x2o+300x2e x 1x0e -> 50x0o+250x0e+50x1o+50x1e+50x2o+50x2e | 97500 paths | 97500 weights)\n",
       "          (lin3): FullyConnectedTensorProduct(100x0o+150x0e+300x1o+250x1e+250x2o+300x2e x 1x0e -> 1x0e | 150 paths | 150 weights)\n",
       "        )\n",
       "        (second): Gate (50x0o+250x0e+50x1o+50x1e+50x2o+50x2e -> 50x0o+50x0e+50x1o+50x1e+50x2o+50x2e)\n",
       "      )\n",
       "      (3): Convolution(\n",
       "        (sc): FullyConnectedTensorProduct(50x0o+50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e -> 1x0e | 50 paths | 50 weights)\n",
       "        (lin1): FullyConnectedTensorProduct(50x0o+50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e -> 50x0o+50x0e+50x1o+50x1e+50x2o+50x2e | 15000 paths | 15000 weights)\n",
       "        (fc): FullyConnectedNet[10, 100, 150]\n",
       "        (tp): TensorProduct(50x0o+50x0e+50x1o+50x1e+50x2o+50x2e x 1x0e+1x1o+1x2e -> 150x0e | 150 paths | 150 weights)\n",
       "        (lin2): FullyConnectedTensorProduct(150x0e x 1x0e -> 1x0e | 150 paths | 150 weights)\n",
       "        (lin3): FullyConnectedTensorProduct(150x0e x 1x0e -> 1x0e | 150 paths | 150 weights)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0064],\n",
       "        [-0.0516],\n",
       "        [-0.0541],\n",
       "        [-0.0695],\n",
       "        [-0.1049],\n",
       "        [-0.0162],\n",
       "        [ 0.0566],\n",
       "        [-0.0174],\n",
       "        [-0.0523],\n",
       "        [-0.0584],\n",
       "        [-0.0801],\n",
       "        [ 0.0191],\n",
       "        [ 0.0661],\n",
       "        [-0.0463],\n",
       "        [-0.0485],\n",
       "        [-0.0651],\n",
       "        [-0.1024],\n",
       "        [ 0.0067],\n",
       "        [-0.0836],\n",
       "        [ 0.0083],\n",
       "        [-0.0599],\n",
       "        [-0.0759],\n",
       "        [-0.0260],\n",
       "        [-0.0430],\n",
       "        [-0.0193],\n",
       "        [ 0.0046],\n",
       "        [-0.0371],\n",
       "        [-0.0629],\n",
       "        [-0.0283],\n",
       "        [-0.0727],\n",
       "        [-0.0310],\n",
       "        [-0.0223],\n",
       "        [-0.0641],\n",
       "        [-0.0654],\n",
       "        [-0.0641],\n",
       "        [-0.0114],\n",
       "        [-0.0605],\n",
       "        [ 0.0305],\n",
       "        [-0.0751],\n",
       "        [-0.0410],\n",
       "        [-0.0645],\n",
       "        [-0.0753],\n",
       "        [ 0.1047],\n",
       "        [-0.0302],\n",
       "        [-0.0761],\n",
       "        [ 0.0248],\n",
       "        [-0.0448],\n",
       "        [ 0.0172]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to(device)\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args,epoch, model, data_loader,\n",
    "                      loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        data, labels = batch_data, batch_data[\"energy\"]\n",
    "        data = data.to(device)\n",
    "        # data['batch'] = data['batch'].to('cpu')\n",
    "        # data['pos'] = data['pos'].to(device)\n",
    "        labels = labels.reshape([-1,1])\n",
    "        labels = labels.to(device)\n",
    "        prediction = model(data)\n",
    "        loss = (loss_criterion(prediction, labels)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(prediction, labels)\n",
    "    total_score = np.mean(train_meter.compute_metric(metric))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
    "        epoch + 1, n_epochs, metric, total_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_an_eval_epoch(args, model, data_loader, loss_criterion):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            data, labels = batch_data, batch_data[\"energy\"]\n",
    "            data = data.to(device)\n",
    "            # data['batch'] = data['batch'].to('cpu')\n",
    "            # data['pos'] = data['pos'].to(device)\n",
    "            labels = labels.reshape([-1,1])\n",
    "            labels = labels.to(device)\n",
    "            prediction = model(data)\n",
    "            eval_loss = (loss_criterion(prediction, labels)).mean()\n",
    "            eval_meter.update(prediction, labels)\n",
    "        total_score = np.mean(eval_meter.compute_metric(metric))\n",
    "    return total_score, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/300, training mae 0.5155\n",
      "epoch 1/300, validation mae 0.3896\n",
      "epoch 2/300, training mae 0.3723\n",
      "epoch 2/300, validation mae 0.3533\n",
      "epoch 3/300, training mae 0.3587\n",
      "epoch 3/300, validation mae 0.3454\n",
      "epoch 4/300, training mae 0.3404\n",
      "epoch 4/300, validation mae 0.3246\n",
      "epoch 5/300, training mae 0.3191\n",
      "epoch 5/300, validation mae 0.3008\n",
      "epoch 6/300, training mae 0.2901\n",
      "epoch 6/300, validation mae 0.2779\n",
      "epoch 7/300, training mae 0.2752\n",
      "epoch 7/300, validation mae 0.2663\n",
      "epoch 8/300, training mae 0.2631\n",
      "epoch 8/300, validation mae 0.2516\n",
      "epoch 9/300, training mae 0.2501\n",
      "epoch 9/300, validation mae 0.2480\n",
      "epoch 10/300, training mae 0.2478\n",
      "epoch 10/300, validation mae 0.2421\n",
      "epoch 11/300, training mae 0.2379\n",
      "epoch 11/300, validation mae 0.2357\n",
      "epoch 12/300, training mae 0.2349\n",
      "epoch 12/300, validation mae 0.2365\n",
      "epoch 13/300, training mae 0.2310\n",
      "epoch 13/300, validation mae 0.2342\n",
      "epoch 14/300, training mae 0.2321\n",
      "epoch 14/300, validation mae 0.2271\n",
      "epoch 15/300, training mae 0.2283\n",
      "epoch 15/300, validation mae 0.2277\n",
      "epoch 16/300, training mae 0.2282\n",
      "epoch 16/300, validation mae 0.2266\n",
      "epoch 17/300, training mae 0.2258\n",
      "epoch 17/300, validation mae 0.2289\n",
      "epoch 18/300, training mae 0.2216\n",
      "epoch 18/300, validation mae 0.2320\n",
      "epoch 19/300, training mae 0.2233\n",
      "epoch 19/300, validation mae 0.2255\n",
      "epoch 20/300, training mae 0.2208\n",
      "epoch 20/300, validation mae 0.2212\n",
      "epoch 21/300, training mae 0.2190\n",
      "epoch 21/300, validation mae 0.2260\n",
      "epoch 22/300, training mae 0.2206\n",
      "epoch 22/300, validation mae 0.2203\n",
      "epoch 23/300, training mae 0.2185\n",
      "epoch 23/300, validation mae 0.2266\n",
      "epoch 24/300, training mae 0.2173\n",
      "epoch 24/300, validation mae 0.2286\n",
      "epoch 25/300, training mae 0.2149\n",
      "epoch 25/300, validation mae 0.2424\n",
      "epoch 26/300, training mae 0.2186\n",
      "epoch 26/300, validation mae 0.2289\n",
      "epoch 27/300, training mae 0.2208\n",
      "epoch 27/300, validation mae 0.2270\n",
      "epoch 28/300, training mae 0.2146\n",
      "epoch 28/300, validation mae 0.2205\n",
      "epoch 29/300, training mae 0.2134\n",
      "epoch 29/300, validation mae 0.2209\n",
      "epoch 30/300, training mae 0.2160\n",
      "epoch 30/300, validation mae 0.2326\n",
      "epoch 31/300, training mae 0.2174\n",
      "epoch 31/300, validation mae 0.2177\n",
      "epoch 32/300, training mae 0.2134\n",
      "epoch 32/300, validation mae 0.2294\n",
      "epoch 33/300, training mae 0.2155\n",
      "epoch 33/300, validation mae 0.2295\n",
      "epoch 34/300, training mae 0.2149\n",
      "epoch 34/300, validation mae 0.2250\n",
      "epoch 35/300, training mae 0.2117\n",
      "epoch 35/300, validation mae 0.2221\n",
      "epoch 36/300, training mae 0.2143\n",
      "epoch 36/300, validation mae 0.2214\n",
      "epoch 37/300, training mae 0.2133\n",
      "epoch 37/300, validation mae 0.2226\n",
      "epoch 38/300, training mae 0.2168\n",
      "epoch 38/300, validation mae 0.2413\n",
      "epoch 39/300, training mae 0.2117\n",
      "epoch 39/300, validation mae 0.2164\n",
      "epoch 40/300, training mae 0.2125\n",
      "epoch 40/300, validation mae 0.2182\n",
      "epoch 41/300, training mae 0.2125\n",
      "epoch 41/300, validation mae 0.2204\n",
      "epoch 42/300, training mae 0.2115\n",
      "epoch 42/300, validation mae 0.2201\n",
      "epoch 43/300, training mae 0.2132\n",
      "epoch 43/300, validation mae 0.2163\n",
      "epoch 44/300, training mae 0.2113\n",
      "epoch 44/300, validation mae 0.2173\n",
      "epoch 45/300, training mae 0.2125\n",
      "epoch 45/300, validation mae 0.2215\n",
      "epoch 46/300, training mae 0.2122\n",
      "epoch 46/300, validation mae 0.2184\n",
      "epoch 47/300, training mae 0.2098\n",
      "epoch 47/300, validation mae 0.2270\n",
      "epoch 48/300, training mae 0.2113\n",
      "epoch 48/300, validation mae 0.2276\n",
      "epoch 49/300, training mae 0.2107\n",
      "epoch 49/300, validation mae 0.2172\n",
      "epoch 50/300, training mae 0.2104\n",
      "epoch 50/300, validation mae 0.2224\n",
      "epoch 51/300, training mae 0.2104\n",
      "epoch 51/300, validation mae 0.2164\n",
      "epoch 52/300, training mae 0.2117\n",
      "epoch 52/300, validation mae 0.2258\n",
      "epoch 53/300, training mae 0.2102\n",
      "epoch 53/300, validation mae 0.2298\n",
      "epoch 54/300, training mae 0.2131\n",
      "epoch 54/300, validation mae 0.2196\n",
      "epoch 55/300, training mae 0.2101\n",
      "epoch 55/300, validation mae 0.2181\n",
      "epoch 56/300, training mae 0.2126\n",
      "epoch 56/300, validation mae 0.2292\n",
      "epoch 57/300, training mae 0.2109\n",
      "epoch 57/300, validation mae 0.2158\n",
      "epoch 58/300, training mae 0.2095\n",
      "epoch 58/300, validation mae 0.2247\n",
      "epoch 59/300, training mae 0.2086\n",
      "epoch 59/300, validation mae 0.2211\n",
      "epoch 60/300, training mae 0.2089\n",
      "epoch 60/300, validation mae 0.2244\n",
      "epoch 61/300, training mae 0.2091\n",
      "epoch 61/300, validation mae 0.2140\n",
      "epoch 62/300, training mae 0.2110\n",
      "epoch 62/300, validation mae 0.2174\n",
      "epoch 63/300, training mae 0.2091\n",
      "epoch 63/300, validation mae 0.2236\n",
      "epoch 64/300, training mae 0.2120\n",
      "epoch 64/300, validation mae 0.2188\n",
      "epoch 65/300, training mae 0.2099\n",
      "epoch 65/300, validation mae 0.2192\n",
      "epoch 66/300, training mae 0.2062\n",
      "epoch 66/300, validation mae 0.2157\n",
      "epoch 67/300, training mae 0.2118\n",
      "epoch 67/300, validation mae 0.2193\n",
      "epoch 68/300, training mae 0.2080\n",
      "epoch 68/300, validation mae 0.2310\n",
      "epoch 69/300, training mae 0.2082\n",
      "epoch 69/300, validation mae 0.2139\n",
      "epoch 70/300, training mae 0.2072\n",
      "epoch 70/300, validation mae 0.2176\n",
      "epoch 71/300, training mae 0.2093\n",
      "epoch 71/300, validation mae 0.2212\n",
      "epoch 72/300, training mae 0.2081\n",
      "epoch 72/300, validation mae 0.2353\n",
      "epoch 73/300, training mae 0.2088\n",
      "epoch 73/300, validation mae 0.2225\n",
      "epoch 74/300, training mae 0.2057\n",
      "epoch 74/300, validation mae 0.2165\n",
      "epoch 75/300, training mae 0.2062\n",
      "epoch 75/300, validation mae 0.2156\n",
      "epoch 76/300, training mae 0.2071\n",
      "epoch 76/300, validation mae 0.2177\n",
      "epoch 77/300, training mae 0.2057\n",
      "epoch 77/300, validation mae 0.2172\n",
      "epoch 78/300, training mae 0.2052\n",
      "epoch 78/300, validation mae 0.2142\n",
      "epoch 79/300, training mae 0.2069\n",
      "epoch 79/300, validation mae 0.2156\n",
      "epoch 80/300, training mae 0.2081\n",
      "epoch 80/300, validation mae 0.2132\n",
      "epoch 81/300, training mae 0.2054\n",
      "epoch 81/300, validation mae 0.2160\n",
      "epoch 82/300, training mae 0.2066\n",
      "epoch 82/300, validation mae 0.2149\n",
      "epoch 83/300, training mae 0.2078\n",
      "epoch 83/300, validation mae 0.2134\n",
      "epoch 84/300, training mae 0.2061\n",
      "epoch 84/300, validation mae 0.2140\n",
      "epoch 85/300, training mae 0.2041\n",
      "epoch 85/300, validation mae 0.2161\n",
      "epoch 86/300, training mae 0.2058\n",
      "epoch 86/300, validation mae 0.2130\n",
      "epoch 87/300, training mae 0.2062\n",
      "epoch 87/300, validation mae 0.2159\n",
      "epoch 88/300, training mae 0.2055\n",
      "epoch 88/300, validation mae 0.2160\n",
      "epoch 89/300, training mae 0.2045\n",
      "epoch 89/300, validation mae 0.2147\n",
      "epoch 90/300, training mae 0.2053\n",
      "epoch 90/300, validation mae 0.2173\n",
      "epoch 91/300, training mae 0.2042\n",
      "epoch 91/300, validation mae 0.2202\n",
      "epoch 92/300, training mae 0.2050\n",
      "epoch 92/300, validation mae 0.2190\n",
      "epoch 93/300, training mae 0.2026\n",
      "epoch 93/300, validation mae 0.2201\n",
      "epoch 94/300, training mae 0.2053\n",
      "epoch 94/300, validation mae 0.2296\n",
      "epoch 95/300, training mae 0.2063\n",
      "epoch 95/300, validation mae 0.2150\n",
      "epoch 96/300, training mae 0.2066\n",
      "epoch 96/300, validation mae 0.2144\n",
      "epoch 97/300, training mae 0.2031\n",
      "epoch 97/300, validation mae 0.2161\n",
      "epoch 98/300, training mae 0.2057\n",
      "epoch 98/300, validation mae 0.2187\n",
      "epoch 99/300, training mae 0.2052\n",
      "epoch 99/300, validation mae 0.2132\n",
      "epoch 100/300, training mae 0.2015\n",
      "epoch 100/300, validation mae 0.2172\n",
      "epoch 101/300, training mae 0.2032\n",
      "epoch 101/300, validation mae 0.2154\n",
      "epoch 102/300, training mae 0.2010\n",
      "epoch 102/300, validation mae 0.2164\n",
      "epoch 103/300, training mae 0.2060\n",
      "epoch 103/300, validation mae 0.2226\n",
      "epoch 104/300, training mae 0.2040\n",
      "epoch 104/300, validation mae 0.2141\n",
      "epoch 105/300, training mae 0.2021\n",
      "epoch 105/300, validation mae 0.2147\n",
      "epoch 106/300, training mae 0.2029\n",
      "epoch 106/300, validation mae 0.2189\n",
      "epoch 107/300, training mae 0.2024\n",
      "epoch 107/300, validation mae 0.2153\n",
      "epoch 108/300, training mae 0.2030\n",
      "epoch 108/300, validation mae 0.2175\n",
      "epoch 109/300, training mae 0.2021\n",
      "epoch 109/300, validation mae 0.2135\n",
      "epoch 110/300, training mae 0.2058\n",
      "epoch 110/300, validation mae 0.2142\n",
      "epoch 111/300, training mae 0.2021\n",
      "epoch 111/300, validation mae 0.2126\n",
      "epoch 112/300, training mae 0.2037\n",
      "epoch 112/300, validation mae 0.2258\n",
      "epoch 113/300, training mae 0.2033\n",
      "epoch 113/300, validation mae 0.2173\n",
      "epoch 114/300, training mae 0.1994\n",
      "epoch 114/300, validation mae 0.2146\n",
      "epoch 115/300, training mae 0.2010\n",
      "epoch 115/300, validation mae 0.2131\n",
      "epoch 116/300, training mae 0.2047\n",
      "epoch 116/300, validation mae 0.2206\n",
      "epoch 117/300, training mae 0.2011\n",
      "epoch 117/300, validation mae 0.2168\n",
      "epoch 118/300, training mae 0.2002\n",
      "epoch 118/300, validation mae 0.2143\n",
      "epoch 119/300, training mae 0.2022\n",
      "epoch 119/300, validation mae 0.2145\n",
      "epoch 120/300, training mae 0.2039\n",
      "epoch 120/300, validation mae 0.2166\n",
      "epoch 121/300, training mae 0.2033\n",
      "epoch 121/300, validation mae 0.2146\n",
      "epoch 122/300, training mae 0.2013\n",
      "epoch 122/300, validation mae 0.2182\n",
      "epoch 123/300, training mae 0.2014\n",
      "epoch 123/300, validation mae 0.2137\n",
      "epoch 124/300, training mae 0.1993\n",
      "epoch 124/300, validation mae 0.2159\n",
      "epoch 125/300, training mae 0.1986\n",
      "epoch 125/300, validation mae 0.2182\n",
      "epoch 126/300, training mae 0.1996\n",
      "epoch 126/300, validation mae 0.2131\n",
      "epoch 127/300, training mae 0.2007\n",
      "epoch 127/300, validation mae 0.2152\n",
      "epoch 128/300, training mae 0.2011\n",
      "epoch 128/300, validation mae 0.2112\n",
      "epoch 129/300, training mae 0.2008\n",
      "epoch 129/300, validation mae 0.2152\n",
      "epoch 130/300, training mae 0.1994\n",
      "epoch 130/300, validation mae 0.2151\n",
      "epoch 131/300, training mae 0.2018\n",
      "epoch 131/300, validation mae 0.2143\n",
      "epoch 132/300, training mae 0.1999\n",
      "epoch 132/300, validation mae 0.2123\n",
      "epoch 133/300, training mae 0.1995\n",
      "epoch 133/300, validation mae 0.2135\n",
      "epoch 134/300, training mae 0.2018\n",
      "epoch 134/300, validation mae 0.2105\n",
      "epoch 135/300, training mae 0.2008\n",
      "epoch 135/300, validation mae 0.2125\n",
      "epoch 136/300, training mae 0.2015\n",
      "epoch 136/300, validation mae 0.2126\n",
      "epoch 137/300, training mae 0.1990\n",
      "epoch 137/300, validation mae 0.2145\n",
      "epoch 138/300, training mae 0.2027\n",
      "epoch 138/300, validation mae 0.2139\n",
      "epoch 139/300, training mae 0.2015\n",
      "epoch 139/300, validation mae 0.2203\n",
      "epoch 140/300, training mae 0.1983\n",
      "epoch 140/300, validation mae 0.2185\n",
      "epoch 141/300, training mae 0.1987\n",
      "epoch 141/300, validation mae 0.2143\n",
      "epoch 142/300, training mae 0.1990\n",
      "epoch 142/300, validation mae 0.2138\n",
      "epoch 143/300, training mae 0.1998\n",
      "epoch 143/300, validation mae 0.2311\n",
      "epoch 144/300, training mae 0.1981\n",
      "epoch 144/300, validation mae 0.2180\n",
      "epoch 145/300, training mae 0.1994\n",
      "epoch 145/300, validation mae 0.2128\n",
      "epoch 146/300, training mae 0.2034\n",
      "epoch 146/300, validation mae 0.2130\n",
      "epoch 147/300, training mae 0.1984\n",
      "epoch 147/300, validation mae 0.2135\n",
      "epoch 148/300, training mae 0.1987\n",
      "epoch 148/300, validation mae 0.2108\n",
      "epoch 149/300, training mae 0.1986\n",
      "epoch 149/300, validation mae 0.2117\n",
      "epoch 150/300, training mae 0.1994\n",
      "epoch 150/300, validation mae 0.2106\n",
      "epoch 151/300, training mae 0.1993\n",
      "epoch 151/300, validation mae 0.2206\n",
      "epoch 152/300, training mae 0.1980\n",
      "epoch 152/300, validation mae 0.2147\n",
      "epoch 153/300, training mae 0.1975\n",
      "epoch 153/300, validation mae 0.2110\n",
      "epoch 154/300, training mae 0.1986\n",
      "epoch 154/300, validation mae 0.2163\n",
      "epoch 155/300, training mae 0.2007\n",
      "epoch 155/300, validation mae 0.2140\n",
      "epoch 156/300, training mae 0.2006\n",
      "epoch 156/300, validation mae 0.2120\n",
      "epoch 157/300, training mae 0.1982\n",
      "epoch 157/300, validation mae 0.2241\n",
      "epoch 158/300, training mae 0.1992\n",
      "epoch 158/300, validation mae 0.2147\n",
      "epoch 159/300, training mae 0.1975\n",
      "epoch 159/300, validation mae 0.2112\n",
      "epoch 160/300, training mae 0.1992\n",
      "epoch 160/300, validation mae 0.2159\n",
      "epoch 161/300, training mae 0.1975\n",
      "epoch 161/300, validation mae 0.2146\n",
      "epoch 162/300, training mae 0.1956\n",
      "epoch 162/300, validation mae 0.2171\n",
      "epoch 163/300, training mae 0.1951\n",
      "epoch 163/300, validation mae 0.2133\n",
      "epoch 164/300, training mae 0.1990\n",
      "epoch 164/300, validation mae 0.2197\n",
      "epoch 165/300, training mae 0.1966\n",
      "epoch 165/300, validation mae 0.2132\n",
      "epoch 166/300, training mae 0.1997\n",
      "epoch 166/300, validation mae 0.2129\n",
      "epoch 167/300, training mae 0.1968\n",
      "epoch 167/300, validation mae 0.2131\n",
      "epoch 168/300, training mae 0.1964\n",
      "epoch 168/300, validation mae 0.2152\n",
      "epoch 169/300, training mae 0.1965\n",
      "epoch 169/300, validation mae 0.2119\n",
      "epoch 170/300, training mae 0.1983\n",
      "epoch 170/300, validation mae 0.2107\n",
      "epoch 171/300, training mae 0.1972\n",
      "epoch 171/300, validation mae 0.2089\n",
      "epoch 172/300, training mae 0.1981\n",
      "epoch 172/300, validation mae 0.2097\n",
      "epoch 173/300, training mae 0.1966\n",
      "epoch 173/300, validation mae 0.2140\n",
      "epoch 174/300, training mae 0.1974\n",
      "epoch 174/300, validation mae 0.2119\n",
      "epoch 175/300, training mae 0.1963\n",
      "epoch 175/300, validation mae 0.2138\n",
      "epoch 176/300, training mae 0.1980\n",
      "epoch 176/300, validation mae 0.2115\n",
      "epoch 177/300, training mae 0.1969\n",
      "epoch 177/300, validation mae 0.2115\n",
      "epoch 178/300, training mae 0.1967\n",
      "epoch 178/300, validation mae 0.2122\n",
      "epoch 179/300, training mae 0.2023\n",
      "epoch 179/300, validation mae 0.2166\n",
      "epoch 180/300, training mae 0.1972\n",
      "epoch 180/300, validation mae 0.2077\n",
      "epoch 181/300, training mae 0.1971\n",
      "epoch 181/300, validation mae 0.2090\n",
      "epoch 182/300, training mae 0.1964\n",
      "epoch 182/300, validation mae 0.2169\n",
      "epoch 183/300, training mae 0.1996\n",
      "epoch 183/300, validation mae 0.2136\n",
      "epoch 184/300, training mae 0.1966\n",
      "epoch 184/300, validation mae 0.2118\n",
      "epoch 185/300, training mae 0.1963\n",
      "epoch 185/300, validation mae 0.2083\n",
      "epoch 186/300, training mae 0.1970\n",
      "epoch 186/300, validation mae 0.2087\n",
      "epoch 187/300, training mae 0.1947\n",
      "epoch 187/300, validation mae 0.2144\n",
      "epoch 188/300, training mae 0.1952\n",
      "epoch 188/300, validation mae 0.2078\n",
      "epoch 189/300, training mae 0.1950\n",
      "epoch 189/300, validation mae 0.2112\n",
      "epoch 190/300, training mae 0.1968\n",
      "epoch 190/300, validation mae 0.2068\n",
      "epoch 191/300, training mae 0.1939\n",
      "epoch 191/300, validation mae 0.2120\n",
      "epoch 192/300, training mae 0.1953\n",
      "epoch 192/300, validation mae 0.2107\n",
      "epoch 193/300, training mae 0.1991\n",
      "epoch 193/300, validation mae 0.2100\n",
      "epoch 194/300, training mae 0.1965\n",
      "epoch 194/300, validation mae 0.2143\n",
      "epoch 195/300, training mae 0.1980\n",
      "epoch 195/300, validation mae 0.2062\n",
      "epoch 196/300, training mae 0.1954\n",
      "epoch 196/300, validation mae 0.2090\n",
      "epoch 197/300, training mae 0.1954\n",
      "epoch 197/300, validation mae 0.2104\n",
      "epoch 198/300, training mae 0.1958\n",
      "epoch 198/300, validation mae 0.2232\n",
      "epoch 199/300, training mae 0.1965\n",
      "epoch 199/300, validation mae 0.2105\n",
      "epoch 200/300, training mae 0.1948\n",
      "epoch 200/300, validation mae 0.2102\n",
      "epoch 201/300, training mae 0.1924\n",
      "epoch 201/300, validation mae 0.2095\n",
      "epoch 202/300, training mae 0.1944\n",
      "epoch 202/300, validation mae 0.2117\n",
      "epoch 203/300, training mae 0.1981\n",
      "epoch 203/300, validation mae 0.2082\n",
      "epoch 204/300, training mae 0.1962\n",
      "epoch 204/300, validation mae 0.2097\n",
      "epoch 205/300, training mae 0.1961\n",
      "epoch 205/300, validation mae 0.2070\n",
      "epoch 206/300, training mae 0.1928\n",
      "epoch 206/300, validation mae 0.2091\n",
      "epoch 207/300, training mae 0.1937\n",
      "epoch 207/300, validation mae 0.2178\n",
      "epoch 208/300, training mae 0.1949\n",
      "epoch 208/300, validation mae 0.2064\n",
      "epoch 209/300, training mae 0.1940\n",
      "epoch 209/300, validation mae 0.2099\n",
      "epoch 210/300, training mae 0.1958\n",
      "epoch 210/300, validation mae 0.2069\n",
      "epoch 211/300, training mae 0.1964\n",
      "epoch 211/300, validation mae 0.2078\n",
      "epoch 212/300, training mae 0.1939\n",
      "epoch 212/300, validation mae 0.2072\n",
      "epoch 213/300, training mae 0.1933\n",
      "epoch 213/300, validation mae 0.2108\n",
      "epoch 214/300, training mae 0.1937\n",
      "epoch 214/300, validation mae 0.2081\n",
      "epoch 215/300, training mae 0.1936\n",
      "epoch 215/300, validation mae 0.2137\n",
      "epoch 216/300, training mae 0.1943\n",
      "epoch 216/300, validation mae 0.2082\n",
      "epoch 217/300, training mae 0.1966\n",
      "epoch 217/300, validation mae 0.2066\n",
      "epoch 218/300, training mae 0.1939\n",
      "epoch 218/300, validation mae 0.2107\n",
      "epoch 219/300, training mae 0.1942\n",
      "epoch 219/300, validation mae 0.2088\n",
      "epoch 220/300, training mae 0.1964\n",
      "epoch 220/300, validation mae 0.2083\n",
      "epoch 221/300, training mae 0.1940\n",
      "epoch 221/300, validation mae 0.2078\n",
      "epoch 222/300, training mae 0.1931\n",
      "epoch 222/300, validation mae 0.2078\n",
      "epoch 223/300, training mae 0.1922\n",
      "epoch 223/300, validation mae 0.2056\n",
      "epoch 224/300, training mae 0.1932\n",
      "epoch 224/300, validation mae 0.2104\n",
      "epoch 225/300, training mae 0.1948\n",
      "epoch 225/300, validation mae 0.2171\n",
      "epoch 226/300, training mae 0.1928\n",
      "epoch 226/300, validation mae 0.2118\n",
      "epoch 227/300, training mae 0.1938\n",
      "epoch 227/300, validation mae 0.2057\n",
      "epoch 228/300, training mae 0.1948\n",
      "epoch 228/300, validation mae 0.2062\n",
      "epoch 229/300, training mae 0.1950\n",
      "epoch 229/300, validation mae 0.2119\n",
      "epoch 230/300, training mae 0.1976\n",
      "epoch 230/300, validation mae 0.2046\n",
      "epoch 231/300, training mae 0.1941\n",
      "epoch 231/300, validation mae 0.2085\n",
      "epoch 232/300, training mae 0.1923\n",
      "epoch 232/300, validation mae 0.2077\n",
      "epoch 233/300, training mae 0.1920\n",
      "epoch 233/300, validation mae 0.2067\n",
      "epoch 234/300, training mae 0.1939\n",
      "epoch 234/300, validation mae 0.2072\n",
      "epoch 235/300, training mae 0.1953\n",
      "epoch 235/300, validation mae 0.2040\n",
      "epoch 236/300, training mae 0.1928\n",
      "epoch 236/300, validation mae 0.2090\n",
      "epoch 237/300, training mae 0.1959\n",
      "epoch 237/300, validation mae 0.2080\n",
      "epoch 238/300, training mae 0.1918\n",
      "epoch 238/300, validation mae 0.2152\n",
      "epoch 239/300, training mae 0.1938\n",
      "epoch 239/300, validation mae 0.2079\n",
      "epoch 240/300, training mae 0.1923\n",
      "epoch 240/300, validation mae 0.2055\n",
      "epoch 241/300, training mae 0.1916\n",
      "epoch 241/300, validation mae 0.2048\n",
      "epoch 242/300, training mae 0.1931\n",
      "epoch 242/300, validation mae 0.2040\n",
      "epoch 243/300, training mae 0.1966\n",
      "epoch 243/300, validation mae 0.2069\n",
      "epoch 244/300, training mae 0.1911\n",
      "epoch 244/300, validation mae 0.2086\n",
      "epoch 245/300, training mae 0.1918\n",
      "epoch 245/300, validation mae 0.2039\n",
      "epoch 246/300, training mae 0.1935\n",
      "epoch 246/300, validation mae 0.2049\n",
      "epoch 247/300, training mae 0.1914\n",
      "epoch 247/300, validation mae 0.2020\n",
      "epoch 248/300, training mae 0.1913\n",
      "epoch 248/300, validation mae 0.2034\n",
      "epoch 249/300, training mae 0.1925\n",
      "epoch 249/300, validation mae 0.2036\n",
      "epoch 250/300, training mae 0.1940\n",
      "epoch 250/300, validation mae 0.2023\n",
      "epoch 251/300, training mae 0.1924\n",
      "epoch 251/300, validation mae 0.2131\n",
      "epoch 252/300, training mae 0.1930\n",
      "epoch 252/300, validation mae 0.2055\n",
      "epoch 253/300, training mae 0.1911\n",
      "epoch 253/300, validation mae 0.2133\n",
      "epoch 254/300, training mae 0.1935\n",
      "epoch 254/300, validation mae 0.2130\n",
      "epoch 255/300, training mae 0.1925\n",
      "epoch 255/300, validation mae 0.2088\n",
      "epoch 256/300, training mae 0.1916\n",
      "epoch 256/300, validation mae 0.2048\n",
      "epoch 257/300, training mae 0.1916\n",
      "epoch 257/300, validation mae 0.2034\n",
      "epoch 258/300, training mae 0.1919\n",
      "epoch 258/300, validation mae 0.2083\n",
      "epoch 259/300, training mae 0.1933\n",
      "epoch 259/300, validation mae 0.2086\n",
      "epoch 260/300, training mae 0.1909\n",
      "epoch 260/300, validation mae 0.2121\n",
      "epoch 261/300, training mae 0.1915\n",
      "epoch 261/300, validation mae 0.2069\n",
      "epoch 262/300, training mae 0.1949\n",
      "epoch 262/300, validation mae 0.2132\n",
      "epoch 263/300, training mae 0.1914\n",
      "epoch 263/300, validation mae 0.2041\n",
      "epoch 264/300, training mae 0.1924\n",
      "epoch 264/300, validation mae 0.2024\n",
      "epoch 265/300, training mae 0.1904\n",
      "epoch 265/300, validation mae 0.2051\n",
      "epoch 266/300, training mae 0.1896\n",
      "epoch 266/300, validation mae 0.2045\n",
      "epoch 267/300, training mae 0.1921\n",
      "epoch 267/300, validation mae 0.2129\n",
      "epoch 268/300, training mae 0.1911\n",
      "epoch 268/300, validation mae 0.2171\n",
      "epoch 269/300, training mae 0.1911\n",
      "epoch 269/300, validation mae 0.2059\n",
      "epoch 270/300, training mae 0.1930\n",
      "epoch 270/300, validation mae 0.2020\n",
      "epoch 271/300, training mae 0.1910\n",
      "epoch 271/300, validation mae 0.2097\n",
      "epoch 272/300, training mae 0.1918\n",
      "epoch 272/300, validation mae 0.2037\n",
      "epoch 273/300, training mae 0.1905\n",
      "epoch 273/300, validation mae 0.2016\n",
      "epoch 274/300, training mae 0.1947\n",
      "epoch 274/300, validation mae 0.2015\n",
      "epoch 275/300, training mae 0.1920\n",
      "epoch 275/300, validation mae 0.2029\n",
      "epoch 276/300, training mae 0.1904\n",
      "epoch 276/300, validation mae 0.1997\n",
      "epoch 277/300, training mae 0.1917\n",
      "epoch 277/300, validation mae 0.2009\n",
      "epoch 278/300, training mae 0.1905\n",
      "epoch 278/300, validation mae 0.2042\n",
      "epoch 279/300, training mae 0.1917\n",
      "epoch 279/300, validation mae 0.2025\n",
      "epoch 280/300, training mae 0.1900\n",
      "epoch 280/300, validation mae 0.2011\n",
      "epoch 281/300, training mae 0.1938\n",
      "epoch 281/300, validation mae 0.1997\n",
      "epoch 282/300, training mae 0.1916\n",
      "epoch 282/300, validation mae 0.1994\n",
      "epoch 283/300, training mae 0.1919\n",
      "epoch 283/300, validation mae 0.2067\n",
      "epoch 284/300, training mae 0.1913\n",
      "epoch 284/300, validation mae 0.2007\n",
      "epoch 285/300, training mae 0.1893\n",
      "epoch 285/300, validation mae 0.2040\n",
      "epoch 286/300, training mae 0.1926\n",
      "epoch 286/300, validation mae 0.2038\n",
      "epoch 287/300, training mae 0.1927\n",
      "epoch 287/300, validation mae 0.2025\n",
      "epoch 288/300, training mae 0.1895\n",
      "epoch 288/300, validation mae 0.1997\n",
      "epoch 289/300, training mae 0.1907\n",
      "epoch 289/300, validation mae 0.2009\n",
      "epoch 290/300, training mae 0.1905\n",
      "epoch 290/300, validation mae 0.2065\n",
      "epoch 291/300, training mae 0.1892\n",
      "epoch 291/300, validation mae 0.2014\n",
      "epoch 292/300, training mae 0.1900\n",
      "epoch 292/300, validation mae 0.2067\n",
      "epoch 293/300, training mae 0.1890\n",
      "epoch 293/300, validation mae 0.2064\n",
      "epoch 294/300, training mae 0.1889\n",
      "epoch 294/300, validation mae 0.2010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    run_a_train_epoch(None, epoch, model, train_loader, loss_fn, optimizer)\n",
    "    val_score, val_loss = run_an_eval_epoch(None, model, val_loader,loss_fn)\n",
    "    print('epoch {:d}/{:d}, validation {} {:.4f}'.format(\n",
    "        epoch + 1, n_epochs, metric, val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score, test_loss = run_an_eval_epoch(None, model, train_loader,loss_fn)\n",
    "print('test {} {:.4f}, test loss {:.4f}'.format(\n",
    "    metric, test_score, test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
