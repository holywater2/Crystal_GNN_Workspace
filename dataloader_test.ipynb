{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division\n",
    "\n",
    "# import csv\n",
    "# import functools\n",
    "# import json\n",
    "# import os\n",
    "# import random\n",
    "# import warnings\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# # from pymatgen.core.structure import Structure\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# def get_train_val_test_loader(dataset, collate_fn=default_collate,\n",
    "#                               batch_size=64, train_ratio=None,\n",
    "#                               val_ratio=0.1, test_ratio=0.1, return_test=False,\n",
    "#                               num_workers=1, pin_memory=False, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Utility function for dividing a dataset to train, val, test datasets.\n",
    "\n",
    "#     !!! The dataset needs to be shuffled before using the function !!!\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset: torch.utils.data.Dataset\n",
    "#       The full dataset to be divided.\n",
    "#     collate_fn: torch.utils.data.DataLoader\n",
    "#     batch_size: int\n",
    "#     train_ratio: float\n",
    "#     val_ratio: float\n",
    "#     test_ratio: float\n",
    "#     return_test: bool\n",
    "#       Whether to return the test dataset loader. If False, the last test_size\n",
    "#       data will be hidden.\n",
    "#     num_workers: int\n",
    "#     pin_memory: bool\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     train_loader: torch.utils.data.DataLoader\n",
    "#       DataLoader that random samples the training data.\n",
    "#     val_loader: torch.utils.data.DataLoader\n",
    "#       DataLoader that random samples the validation data.\n",
    "#     (test_loader): torch.utils.data.DataLoader\n",
    "#       DataLoader that random samples the test data, returns if\n",
    "#         return_test=True.\n",
    "#     \"\"\"\n",
    "#     total_size = len(dataset)\n",
    "#     if kwargs['train_size'] is None:\n",
    "#         if train_ratio is None:\n",
    "#             assert val_ratio + test_ratio < 1\n",
    "#             train_ratio = 1 - val_ratio - test_ratio\n",
    "#             print(f'[Warning] train_ratio is None, using 1 - val_ratio - '\n",
    "#                   f'test_ratio = {train_ratio} as training data.')\n",
    "#         else:\n",
    "#             assert train_ratio + val_ratio + test_ratio <= 1\n",
    "#     indices = list(range(total_size))\n",
    "#     if kwargs['train_size']:\n",
    "#         train_size = kwargs['train_size']\n",
    "#     else:\n",
    "#         train_size = int(train_ratio * total_size)\n",
    "#     if kwargs['test_size']:\n",
    "#         test_size = kwargs['test_size']\n",
    "#     else:\n",
    "#         test_size = int(test_ratio * total_size)\n",
    "#     if kwargs['val_size']:\n",
    "#         valid_size = kwargs['val_size']\n",
    "#     else:\n",
    "#         valid_size = int(val_ratio * total_size)\n",
    "#     train_sampler = SubsetRandomSampler(indices[:train_size])\n",
    "#     val_sampler = SubsetRandomSampler(\n",
    "#         indices[-(valid_size + test_size):-test_size])\n",
    "#     if return_test:\n",
    "#         test_sampler = SubsetRandomSampler(indices[-test_size:])\n",
    "#     train_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "#                               sampler=train_sampler,\n",
    "#                               num_workers=num_workers,\n",
    "#                               collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "#     val_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "#                             sampler=val_sampler,\n",
    "#                             num_workers=num_workers,\n",
    "#                             collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "#     if return_test:\n",
    "#         test_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "#                                  sampler=test_sampler,\n",
    "#                                  num_workers=num_workers,\n",
    "#                                  collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "#     if return_test:\n",
    "#         return train_loader, val_loader, test_loader\n",
    "#     else:\n",
    "#         return train_loader, val_loader\n",
    "\n",
    "\n",
    "# def collate_pool(dataset_list):\n",
    "#     \"\"\"\n",
    "#     Collate a list of data and return a batch for predicting crystal\n",
    "#     properties.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "\n",
    "#     dataset_list: list of tuples for each data point.\n",
    "#       (atom_fea, nbr_fea, nbr_fea_idx, target)\n",
    "\n",
    "#       atom_fea: torch.Tensor shape (n_i, atom_fea_len)\n",
    "#       nbr_fea: torch.Tensor shape (n_i, M, nbr_fea_len)\n",
    "#       nbr_fea_idx: torch.LongTensor shape (n_i, M)\n",
    "#       target: torch.Tensor shape (1, )\n",
    "#       cif_id: str or int\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     N = sum(n_i); N0 = sum(i)\n",
    "\n",
    "#     batch_atom_fea: torch.Tensor shape (N, orig_atom_fea_len)\n",
    "#       Atom features from atom type\n",
    "#     batch_nbr_fea: torch.Tensor shape (N, M, nbr_fea_len)\n",
    "#       Bond features of each atom's M neighbors\n",
    "#     batch_nbr_fea_idx: torch.LongTensor shape (N, M)\n",
    "#       Indices of M neighbors of each atom\n",
    "#     crystal_atom_idx: list of torch.LongTensor of length N0\n",
    "#       Mapping from the crystal idx to atom idx\n",
    "#     target: torch.Tensor shape (N, 1)\n",
    "#       Target value for prediction\n",
    "#     batch_cif_ids: list\n",
    "#     \"\"\"\n",
    "#     batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []\n",
    "#     crystal_atom_idx, batch_target = [], []\n",
    "#     batch_cif_ids = []\n",
    "#     base_idx = 0\n",
    "#     for i, ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id)\\\n",
    "#             in enumerate(dataset_list):\n",
    "#         n_i = atom_fea.shape[0]  # number of atoms for this crystal\n",
    "#         batch_atom_fea.append(atom_fea)\n",
    "#         batch_nbr_fea.append(nbr_fea)\n",
    "#         batch_nbr_fea_idx.append(nbr_fea_idx+base_idx)\n",
    "#         new_idx = torch.LongTensor(np.arange(n_i)+base_idx)\n",
    "#         crystal_atom_idx.append(new_idx)\n",
    "#         batch_target.append(target)\n",
    "#         batch_cif_ids.append(cif_id)\n",
    "#         base_idx += n_i\n",
    "#     return (torch.cat(batch_atom_fea, dim=0),\n",
    "#             torch.cat(batch_nbr_fea, dim=0),\n",
    "#             torch.cat(batch_nbr_fea_idx, dim=0),\n",
    "#             crystal_atom_idx),\\\n",
    "#         torch.stack(batch_target, dim=0),\\\n",
    "#         batch_cif_ids\n",
    "\n",
    "\n",
    "# class GaussianDistance(object):\n",
    "#     \"\"\"\n",
    "#     Expands the distance by Gaussian basis.\n",
    "\n",
    "#     Unit: angstrom\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dmin, dmax, step, var=None):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "\n",
    "#         dmin: float\n",
    "#           Minimum interatomic distance\n",
    "#         dmax: float\n",
    "#           Maximum interatomic distance\n",
    "#         step: float\n",
    "#           Step size for the Gaussian filter\n",
    "#         \"\"\"\n",
    "#         assert dmin < dmax\n",
    "#         assert dmax - dmin > step\n",
    "#         self.filter = np.arange(dmin, dmax+step, step)\n",
    "#         if var is None:\n",
    "#             var = step\n",
    "#         self.var = var\n",
    "\n",
    "#     def expand(self, distances):\n",
    "#         \"\"\"\n",
    "#         Apply Gaussian disntance filter to a numpy distance array\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "\n",
    "#         distance: np.array shape n-d array\n",
    "#           A distance matrix of any shape\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         expanded_distance: shape (n+1)-d array\n",
    "#           Expanded distance matrix with the last dimension of length\n",
    "#           len(self.filter)\n",
    "#         \"\"\"\n",
    "#         return np.exp(-(distances[..., np.newaxis] - self.filter)**2 /\n",
    "#                       self.var**2)\n",
    "\n",
    "\n",
    "# class AtomInitializer(object):\n",
    "#     \"\"\"\n",
    "#     Base class for intializing the vector representation for atoms.\n",
    "\n",
    "#     !!! Use one AtomInitializer per dataset !!!\n",
    "#     \"\"\"\n",
    "#     def __init__(self, atom_types):\n",
    "#         self.atom_types = set(atom_types)\n",
    "#         self._embedding = {}\n",
    "\n",
    "#     def get_atom_fea(self, atom_type):\n",
    "#         assert atom_type in self.atom_types\n",
    "#         return self._embedding[atom_type]\n",
    "\n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         self._embedding = state_dict\n",
    "#         self.atom_types = set(self._embedding.keys())\n",
    "#         self._decodedict = {idx: atom_type for atom_type, idx in\n",
    "#                             self._embedding.items()}\n",
    "\n",
    "#     def state_dict(self):\n",
    "#         return self._embedding\n",
    "\n",
    "#     def decode(self, idx):\n",
    "#         if not hasattr(self, '_decodedict'):\n",
    "#             self._decodedict = {idx: atom_type for atom_type, idx in\n",
    "#                                 self._embedding.items()}\n",
    "#         return self._decodedict[idx]\n",
    "\n",
    "\n",
    "# class AtomCustomJSONInitializer(AtomInitializer):\n",
    "#     \"\"\"\n",
    "#     Initialize atom feature vectors using a JSON file, which is a python\n",
    "#     dictionary mapping from element number to a list representing the\n",
    "#     feature vector of the element.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "\n",
    "#     elem_embedding_file: str\n",
    "#         The path to the .json file\n",
    "#     \"\"\"\n",
    "#     def __init__(self, elem_embedding_file):\n",
    "#         with open(elem_embedding_file) as f:\n",
    "#             elem_embedding = json.load(f)\n",
    "#         elem_embedding = {int(key): value for key, value\n",
    "#                           in elem_embedding.items()}\n",
    "#         atom_types = set(elem_embedding.keys())\n",
    "#         super(AtomCustomJSONInitializer, self).__init__(atom_types)\n",
    "#         for key, value in elem_embedding.items():\n",
    "#             self._embedding[key] = np.array(value, dtype=float)\n",
    "\n",
    "\n",
    "# class CIFData(Dataset):\n",
    "#     \"\"\"\n",
    "#     The CIFData dataset is a wrapper for a dataset where the crystal structures\n",
    "#     are stored in the form of CIF files. The dataset should have the following\n",
    "#     directory structure:\n",
    "\n",
    "#     root_dir\n",
    "#     ├── id_prop.csv\n",
    "#     ├── atom_init.json\n",
    "#     ├── id0.cif\n",
    "#     ├── id1.cif\n",
    "#     ├── ...\n",
    "\n",
    "#     id_prop.csv: a CSV file with two columns. The first column recodes a\n",
    "#     unique ID for each crystal, and the second column recodes the value of\n",
    "#     target property.\n",
    "\n",
    "#     atom_init.json: a JSON file that stores the initialization vector for each\n",
    "#     element.\n",
    "\n",
    "#     ID.cif: a CIF file that recodes the crystal structure, where ID is the\n",
    "#     unique ID for the crystal.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "\n",
    "#     root_dir: str\n",
    "#         The path to the root directory of the dataset\n",
    "#     max_num_nbr: int\n",
    "#         The maximum number of neighbors while constructing the crystal graph\n",
    "#     radius: float\n",
    "#         The cutoff radius for searching neighbors\n",
    "#     dmin: float\n",
    "#         The minimum distance for constructing GaussianDistance\n",
    "#     step: float\n",
    "#         The step size for constructing GaussianDistance\n",
    "#     random_seed: int\n",
    "#         Random seed for shuffling the dataset\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "\n",
    "#     atom_fea: torch.Tensor shape (n_i, atom_fea_len)\n",
    "#     nbr_fea: torch.Tensor shape (n_i, M, nbr_fea_len)\n",
    "#     nbr_fea_idx: torch.LongTensor shape (n_i, M)\n",
    "#     target: torch.Tensor shape (1, )\n",
    "#     cif_id: str or int\n",
    "#     \"\"\"\n",
    "#     def __init__(self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2,\n",
    "#                  random_seed=123):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.max_num_nbr, self.radius = max_num_nbr, radius\n",
    "#         assert os.path.exists(root_dir), 'root_dir does not exist!'\n",
    "#         id_prop_file = os.path.join(self.root_dir, 'id_prop.csv')\n",
    "#         assert os.path.exists(id_prop_file), 'id_prop.csv does not exist!'\n",
    "#         with open(id_prop_file) as f:\n",
    "#             reader = csv.reader(f)\n",
    "#             self.id_prop_data = [row for row in reader]\n",
    "#         random.seed(random_seed)\n",
    "#         random.shuffle(self.id_prop_data)\n",
    "#         atom_init_file = os.path.join(self.root_dir, 'atom_init.json')\n",
    "#         assert os.path.exists(atom_init_file), 'atom_init.json does not exist!'\n",
    "#         self.ari = AtomCustomJSONInitializer(atom_init_file)\n",
    "#         self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.id_prop_data)\n",
    "\n",
    "#     @functools.lru_cache(maxsize=None)  # Cache loaded structures\n",
    "#     def __getitem__(self, idx):\n",
    "#         cif_id, target = self.id_prop_data[idx]\n",
    "#         crystal = Structure.from_file(os.path.join(self.root_dir,\n",
    "#                                                    cif_id+'.cif'))\n",
    "#         atom_fea = np.vstack([self.ari.get_atom_fea(crystal[i].specie.number)\n",
    "#                               for i in range(len(crystal))])\n",
    "#         atom_fea = torch.Tensor(atom_fea)\n",
    "#         all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n",
    "#         all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n",
    "#         nbr_fea_idx, nbr_fea = [], []\n",
    "#         for nbr in all_nbrs:\n",
    "#             if len(nbr) < self.max_num_nbr:\n",
    "#                 warnings.warn('{} not find enough neighbors to build graph. '\n",
    "#                               'If it happens frequently, consider increase '\n",
    "#                               'radius.'.format(cif_id))\n",
    "#                 nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) +\n",
    "#                                    [0] * (self.max_num_nbr - len(nbr)))\n",
    "#                 nbr_fea.append(list(map(lambda x: x[1], nbr)) +\n",
    "#                                [self.radius + 1.] * (self.max_num_nbr -\n",
    "#                                                      len(nbr)))\n",
    "#             else:\n",
    "#                 nbr_fea_idx.append(list(map(lambda x: x[2],\n",
    "#                                             nbr[:self.max_num_nbr])))\n",
    "#                 nbr_fea.append(list(map(lambda x: x[1],\n",
    "#                                         nbr[:self.max_num_nbr])))\n",
    "#         nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n",
    "#         nbr_fea = self.gdf.expand(nbr_fea)\n",
    "#         atom_fea = torch.Tensor(atom_fea)\n",
    "#         nbr_fea = torch.Tensor(nbr_fea)\n",
    "#         nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n",
    "#         target = torch.Tensor([float(target)])\n",
    "#         return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holywater2/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import repo_utils.data_utils as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp.mp.load_json(down=False,downsave=False,pdirname=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete: the number of the loaded data is 6923\n"
     ]
    }
   ],
   "source": [
    "sampled_data = pp.mp.load_json(pdirname=\"dataset\",dirname=\"mp_megnet_sample\",down=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
